"""
main.py ‚Äî FastAPI entrypoint for Nexus.

Fully documented REST API with Swagger UI at /docs and ReDoc at /redoc.

Endpoints:
  - GET  /api/health     ‚Üí System status & API key check
  - GET  /api/skills     ‚Üí List generated skill manifests
  - GET  /api/skills/{name} ‚Üí Get a specific skill manifest content
  - POST /api/discover   ‚Üí Run full Discovery pipeline (ingest ‚Üí council ‚Üí SKILLS.md)
  - POST /api/chat       ‚Üí Skill-aware chat (RAG + Skill Manifest)
  - DELETE /api/skills/{name} ‚Üí Delete a skill manifest
"""

from __future__ import annotations

import asyncio
import sys
from pathlib import Path
from typing import Any, Optional

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.utils import get_openapi
from pydantic import BaseModel, Field

# Ensure src/ is importable
_project_root = Path(__file__).resolve().parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

load_dotenv()

from src.ingestion.cast_parser import parse_file
from src.ingestion.chunker import SemanticChunkerPipeline
from src.ingestion.connectors import ConfluenceConnector, GitHubConnector, MultiRepoConnector
from src.knowledge.vector_store import VectorStore
from src.orchestration.adk_core import (
    AgentResponse,
    ChatRequest,
    ChatResponse,
    DiscoveryResult,
    validate_api_keys,
)
from src.orchestration.chat_agent import answer
from src.orchestration.council_agents import run_council
from src.skills.registry import SkillRegistry


# ---------------------------------------------------------------------------
# API Tags (for Swagger grouping)
# ---------------------------------------------------------------------------

tags_metadata = [
    {
        "name": "System",
        "description": "Health checks and system status. Use these to verify your setup.",
    },
    {
        "name": "Skills",
        "description": (
            "Manage Skill Manifests. A **Skill** is a domain-specific knowledge file "
            "(SKILLS.md) generated by the LLM Council. Each skill represents a different "
            '"hat" the AI can wear ‚Äî e.g., "staking", "authentication", "payments".'
        ),
    },
    {
        "name": "Discovery",
        "description": (
            "Run the Discovery pipeline to analyze code & docs and generate a Skill Manifest. "
            "This is a **long-running** operation (~60-90 seconds) that:\n\n"
            "1. Ingests files from `data/code/` and `data/docs/`\n"
            "2. Parses code with Tree-sitter (extracts methods/functions)\n"
            "3. Creates semantic chunks and embeds them in ChromaDB\n"
            "4. Runs the **LLM Council** (Architect, Domain Expert, Quality Agent)\n"
            "5. Synthesizes results into a structured SKILLS.md manifest"
        ),
    },
    {
        "name": "Chat",
        "description": (
            "Skill-aware conversational AI. Select a Skill Hat and ask questions. "
            "The AI grounds its answers in:\n\n"
            "- The skill's **SKILLS.md manifest** (patterns, rules, standards)\n"
            "- **Relevant code chunks** retrieved from ChromaDB (RAG)\n"
            "- **Conversation history** for multi-turn context"
        ),
    },
]


# ---------------------------------------------------------------------------
# Application Setup
# ---------------------------------------------------------------------------

app = FastAPI(
    title="üß† Nexus API",
    description=(
        "**Skill-Aware AI Developer Assistant** ‚Äî An enterprise Agentic RAG system.\n\n"
        "Nexus analyzes your codebase using an LLM Council (Architect, Domain Expert, Quality Agent) "
        "to generate structured **Skill Manifests**. Developers can then chat with the AI using a specific "
        '"Skill Hat" to get answers grounded in their codebase\'s patterns, rules, and standards.\n\n'
        "---\n\n"
        "### üöÄ Quick Start\n\n"
        "1. **Check health**: `GET /api/health` ‚Äî Verify your API keys are set\n"
        "2. **Add data**: Place code in `data/code/`, docs in `data/docs/`\n"
        "3. **Run discovery**: `POST /api/discover` ‚Äî Generate a Skill Manifest\n"
        "4. **Chat**: `POST /api/chat` ‚Äî Ask questions using your new Skill Hat\n\n"
        "---\n\n"
        "### üìñ Terminology\n\n"
        "| Term | Meaning |\n"
        "|------|--------|\n"
        "| **Skill** | A domain-specific knowledge manifest (e.g., staking, auth) |\n"
        "| **Skill Hat** | The active skill the AI uses to answer questions |\n"
        "| **SKILLS.md** | The generated Markdown file with patterns, rules, deviations |\n"
        "| **LLM Council** | Three AI agents that analyze your code in parallel |\n"
        "| **Discovery** | The pipeline that ingests code ‚Üí generates SKILLS.md |\n"
    ),
    version="0.1.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_tags=tags_metadata,
    contact={
        "name": "Nexus",
    },
    license_info={
        "name": "MIT",
    },
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ---------------------------------------------------------------------------
# Request/Response Models (with Swagger examples)
# ---------------------------------------------------------------------------

class DiscoverRequest(BaseModel):
    """Request body to trigger the Discovery pipeline."""

    data_path: str = Field(
        default="data",
        description=(
            "Path to the data folder relative to the project root. "
            "Must contain `code/` and/or `docs/` subfolders."
        ),
        json_schema_extra={"examples": ["data"]},
    )
    skill_name: str = Field(
        ...,
        description=(
            "Name for the generated Skill Manifest. Use a short, descriptive name "
            "for the domain you're analyzing (e.g., 'staking', 'auth', 'payments')."
        ),
        min_length=1,
        max_length=50,
        json_schema_extra={"examples": ["staking"]},
    )
    code_extensions: list[str] = Field(
        default_factory=lambda: [".java", ".ts", ".tsx"],
        description="File extensions to include when scanning for code files.",
        json_schema_extra={"examples": [[".java", ".ts", ".tsx"]]},
    )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "data_path": "data",
                    "skill_name": "staking",
                    "code_extensions": [".java", ".ts", ".tsx"],
                }
            ]
        }
    }

class RepoInput(BaseModel):
    """Configuration for a single repository to analyze."""

    url: str = Field(
        ...,
        description=(
            "Full Git clone URL. Supports HTTPS and SSH.\n\n"
            "Examples:\n"
            "- `https://github.com/owner/repo`\n"
            "- `git@github.com:owner/repo.git`"
        ),
        json_schema_extra={"examples": ["https://github.com/owner/my-app"]},
    )
    branch: str = Field(
        default="",
        description="Branch to clone. Leave empty for the default branch.",
    )
    subdirectory: str = Field(
        default="",
        description="Optional subdirectory to focus on (e.g., 'src/main/java' for monorepos).",
    )


class DiscoverRepoRequest(BaseModel):
    """Request body to trigger Discovery from one or more Git repositories."""

    repos: list[RepoInput] = Field(
        ...,
        description=(
            "One or more Git repositories to analyze. All repos are cloned, "
            "scanned, and their files merged before running the LLM Council."
        ),
        min_length=1,
    )
    skill_name: str = Field(
        ...,
        description="Name for the generated Skill Manifest (e.g., 'staking', 'auth').",
        min_length=1,
        max_length=50,
        json_schema_extra={"examples": ["staking"]},
    )
    code_extensions: list[str] = Field(
        default_factory=lambda: [".java", ".ts", ".tsx"],
        description="Code file extensions to include.",
    )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "repos": [
                        {"url": "https://github.com/org/backend", "branch": "main"},
                        {"url": "https://github.com/org/shared-lib"},
                    ],
                    "skill_name": "staking",
                    "code_extensions": [".java", ".ts", ".tsx"],
                }
            ]
        }
    }


class SkillListResponse(BaseModel):
    """Response listing all available skill manifests."""

    skills: list[str] = Field(
        description="List of skill names that have been generated."
    )
    count: int = Field(description="Total number of available skills.")

    model_config = {
        "json_schema_extra": {
            "examples": [
                {"skills": ["staking", "authentication", "payments"], "count": 3}
            ]
        }
    }


class SkillDetailResponse(BaseModel):
    """Response containing a single skill manifest's content."""

    skill_name: str = Field(description="Name of the skill.")
    content: str = Field(description="Full Markdown content of the SKILLS.md manifest.")
    exists: bool = Field(description="Whether the skill manifest exists.")

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "skill_name": "staking",
                    "content": "# STAKING ‚Äî Skill Manifest\n\n## Overview\n...",
                    "exists": True,
                }
            ]
        }
    }


class SkillDeleteResponse(BaseModel):
    """Response after deleting a skill manifest."""

    skill_name: str = Field(description="Name of the deleted skill.")
    deleted: bool = Field(description="Whether the deletion was successful.")
    message: str = Field(description="Human-readable status message.")


class HealthResponse(BaseModel):
    """System health check response."""

    status: str = Field(default="ok", description="System status: 'ok' or 'error'.")
    api_keys: dict[str, bool] = Field(
        description=(
            "Status of configured API keys. "
            "`true` = key is set in `.env`, `false` = missing."
        )
    )
    skills_count: int = Field(
        description="Number of skill manifests currently generated."
    )
    data_folder: dict[str, Any] = Field(
        default_factory=dict,
        description="Info about the data folder (file counts, etc.).",
    )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "status": "ok",
                    "api_keys": {
                        "google": True,
                        "anthropic": False,
                        "github": False,
                        "confluence": False,
                    },
                    "skills_count": 2,
                    "data_folder": {
                        "code_files": 12,
                        "doc_files": 5,
                    },
                }
            ]
        }
    }


class ErrorResponse(BaseModel):
    """Standard error response."""

    detail: str = Field(description="Human-readable error message.")

    model_config = {
        "json_schema_extra": {
            "examples": [
                {"detail": "Skill 'staking' not found. Run discovery first."}
            ]
        }
    }


# ---------------------------------------------------------------------------
# Helper
# ---------------------------------------------------------------------------

def _count_data_files(data_path: str = "data") -> dict[str, int]:
    """Count files in data/code/ and data/docs/."""
    code_dir = Path(data_path) / "code"
    docs_dir = Path(data_path) / "docs"

    code_count = 0
    doc_count = 0

    if code_dir.exists():
        code_count = sum(
            1 for _ in code_dir.rglob("*")
            if _.is_file() and _.suffix in {".java", ".ts", ".tsx", ".js", ".py"}
        )
    if docs_dir.exists():
        doc_count = sum(
            1 for _ in docs_dir.rglob("*")
            if _.is_file() and _.suffix in {".md", ".html", ".txt", ".rst"}
        )

    return {"code_files": code_count, "doc_files": doc_count}


# ---------------------------------------------------------------------------
# System Endpoints
# ---------------------------------------------------------------------------

@app.get(
    "/api/health",
    response_model=HealthResponse,
    tags=["System"],
    summary="Health Check",
    description=(
        "Check system health, including which API keys are configured and "
        "how many skills have been generated. Use this to verify your `.env` is set up correctly.\n\n"
        "**Required API keys:**\n"
        "- `GOOGLE_API_KEY` ‚Äî Needed for all LLM operations\n\n"
        "**Optional API keys:**\n"
        "- `GITHUB_TOKEN` ‚Äî For live GitHub repo ingestion (otherwise uses local `data/code/`)\n"
        "- `CONFLUENCE_URL` + `CONFLUENCE_TOKEN` ‚Äî For live Confluence page ingestion"
    ),
)
async def health_check() -> HealthResponse:
    registry = SkillRegistry()
    return HealthResponse(
        api_keys=validate_api_keys(),
        skills_count=len(registry.list_skills()),
        data_folder=_count_data_files(),
    )


# ---------------------------------------------------------------------------
# Skills Endpoints
# ---------------------------------------------------------------------------

@app.get(
    "/api/skills",
    response_model=SkillListResponse,
    tags=["Skills"],
    summary="List All Skills",
    description=(
        "List all available skill manifests that have been generated via the Discovery pipeline.\n\n"
        "Each skill name can be used as a `skill_hat` in the `/api/chat` endpoint."
    ),
)
async def list_skills() -> SkillListResponse:
    registry = SkillRegistry()
    skills = registry.list_skills()
    return SkillListResponse(skills=skills, count=len(skills))


@app.get(
    "/api/skills/{skill_name}",
    response_model=SkillDetailResponse,
    tags=["Skills"],
    summary="Get Skill Manifest",
    description=(
        "Retrieve the full SKILLS.md content for a specific skill.\n\n"
        "The manifest contains:\n"
        "- Architecture Patterns\n"
        "- Entity Map\n"
        "- Business Logic & Rules\n"
        "- Code Standards & Quality Baseline\n"
        "- Quality Deviations & Corrections\n"
        "- Implementation Guidelines (DO/DON'T)\n"
        "- Code Examples"
    ),
    responses={
        404: {"model": ErrorResponse, "description": "Skill not found."},
    },
)
async def get_skill(skill_name: str) -> SkillDetailResponse:
    registry = SkillRegistry()
    content = registry.load_skill(skill_name)
    if not content:
        raise HTTPException(
            status_code=404,
            detail=f"Skill '{skill_name}' not found. Run discovery with this skill name first.",
        )
    return SkillDetailResponse(
        skill_name=skill_name,
        content=content,
        exists=True,
    )


@app.delete(
    "/api/skills/{skill_name}",
    response_model=SkillDeleteResponse,
    tags=["Skills"],
    summary="Delete Skill Manifest",
    description="Delete a skill manifest. This does **not** remove the ChromaDB embeddings.",
    responses={
        404: {"model": ErrorResponse, "description": "Skill not found."},
    },
)
async def delete_skill(skill_name: str) -> SkillDeleteResponse:
    registry = SkillRegistry()
    deleted = registry.delete_skill(skill_name)
    if not deleted:
        raise HTTPException(
            status_code=404,
            detail=f"Skill '{skill_name}' not found.",
        )
    return SkillDeleteResponse(
        skill_name=skill_name,
        deleted=True,
        message=f"Skill '{skill_name}' has been deleted.",
    )


# ---------------------------------------------------------------------------
# Discovery Endpoint
# ---------------------------------------------------------------------------

@app.post(
    "/api/discover",
    response_model=DiscoveryResult,
    tags=["Discovery"],
    summary="Run Discovery Pipeline",
    description=(
        "Trigger the full Discovery pipeline to analyze your codebase and generate a Skill Manifest.\n\n"
        "### ‚è±Ô∏è Expected Duration: 60‚Äì90 seconds\n\n"
        "### Pipeline Steps:\n\n"
        "| Step | Action | Details |\n"
        "|------|--------|---------|\n"
        "| 1 | **Ingest** | Read files from `data/code/` and `data/docs/` |\n"
        "| 2 | **Parse** | Extract methods/functions via Tree-sitter AST parsing |\n"
        "| 3 | **Chunk** | Create semantic chunks for embedding |\n"
        "| 4 | **Embed** | Store chunks in ChromaDB with vector embeddings |\n"
        "| 5 | **Council** | Run 3 AI agents in parallel (Architect, Domain, Quality) |\n"
        "| 6 | **Synthesize** | Merge agent analyses into SKILLS.md |\n\n"
        "### Prerequisites:\n"
        "- `GOOGLE_API_KEY` must be set in `.env`\n"
        "- Files must exist in `data/code/` and/or `data/docs/`\n\n"
        "### Response:\n"
        "The response includes the generated SKILLS.md content, individual agent analyses, "
        "and the number of chunks ingested into the vector store."
    ),
    responses={
        500: {"model": ErrorResponse, "description": "Pipeline error (check detail for specifics)."},
    },
)
async def discover(request: DiscoverRequest) -> DiscoveryResult:
    try:
        # --- Step 1: Ingest ---
        code_connector = GitHubConnector(local_dir=f"{request.data_path}/code")
        doc_connector = ConfluenceConnector(local_dir=f"{request.data_path}/docs")

        extensions = {ext if ext.startswith(".") else f".{ext}" for ext in request.code_extensions}

        code_docs = await code_connector.fetch_files(extensions=extensions)
        doc_docs = await doc_connector.fetch_pages()

        if not code_docs and not doc_docs:
            raise HTTPException(
                status_code=400,
                detail=(
                    f"No files found in '{request.data_path}/code/' or '{request.data_path}/docs/'. "
                    "Please add your source files before running discovery."
                ),
            )

        # --- Step 2: Parse ---
        all_code_chunks = []
        for doc in code_docs:
            file_path = doc.metadata.get("local_path", doc.source)
            chunks = parse_file(doc.content, file_path)
            all_code_chunks.extend(chunks)

        # --- Step 3: Chunk ---
        chunker = SemanticChunkerPipeline()
        semantic_code = chunker.chunk_code(all_code_chunks)
        semantic_docs = []
        for doc in doc_docs:
            semantic_docs.extend(
                chunker.chunk_document(doc.content, doc.source, doc.file_type)
            )

        all_semantic = semantic_code + semantic_docs

        # --- Step 4: Embed in ChromaDB ---
        store = VectorStore()
        chunks_to_upsert = [
            {"id": c.id, "content": c.content, "metadata": c.metadata}
            for c in all_semantic
        ]
        stored = store.upsert_chunks(request.skill_name, chunks_to_upsert)

        # --- Step 5: Run Council ---
        combined_text = "\n\n---\n\n".join(
            c.content for c in all_semantic[:50]  # Cap for LLM context window
        )
        result = await run_council(combined_text, request.skill_name)
        result.chunks_ingested = stored

        return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Discovery pipeline error: {str(e)}")


@app.post(
    "/api/discover-repo",
    response_model=DiscoveryResult,
    tags=["Discovery"],
    summary="Discover from Git Repo URL(s) ‚≠ê",
    description=(
        "**Recommended method.** Provide one or more Git repository URLs and Nexus will:\n\n"
        "1. **Clone** each repo (shallow, depth=1 for speed)\n"
        "2. **Scan** all repos for code and documentation files\n"
        "3. **Merge** files from all repos into a single analysis\n"
        "4. **Parse** code with Tree-sitter\n"
        "5. **Chunk & embed** into ChromaDB\n"
        "6. **Run the LLM Council** (Architect, Domain Expert, Quality Agent)\n"
        "7. **Generate** a structured SKILLS.md manifest\n\n"
        "### ‚è±Ô∏è Expected Duration: 60‚Äì120 seconds\n\n"
        "### Multi-Repo Support:\n"
        "Combine a backend + frontend + shared library into one skill:\n"
        "```json\n"
        '{\n'
        '  "repos": [\n'
        '    {"url": "https://github.com/org/backend", "branch": "main"},\n'
        '    {"url": "https://github.com/org/frontend", "subdirectory": "src"},\n'
        '    {"url": "git@github.com:org/shared-lib.git"}\n'
        '  ],\n'
        '  "skill_name": "staking"\n'
        '}\n'
        "```\n\n"
        "### Private Repos:\n"
        "- **SSH**: Use `git@github.com:owner/repo.git` (requires SSH keys on server)\n"
        "- **HTTPS + PAT**: Set `GITHUB_TOKEN` in `.env` ‚Äî it is auto-injected into HTTPS URLs\n\n"
        "All cloned repos are automatically cleaned up after processing."
    ),
    responses={
        400: {"model": ErrorResponse, "description": "Clone failed or no files found."},
        500: {"model": ErrorResponse, "description": "Pipeline error."},
    },
)
async def discover_repo(request: DiscoverRepoRequest) -> DiscoveryResult:
    connector: MultiRepoConnector | None = None
    try:
        # --- Step 1: Clone all repos ---
        repo_configs = [
            {
                "url": r.url,
                "branch": r.branch,
                "subdirectory": r.subdirectory,
            }
            for r in request.repos
        ]
        connector = MultiRepoConnector(repos=repo_configs)

        clone_results = connector.clone_all()

        # Check if any repos succeeded
        succeeded = [r for r in clone_results if r["success"]]
        failed = [r for r in clone_results if not r["success"]]

        if not succeeded:
            errors = "; ".join(f"{r['repo_url']}: {r['error']}" for r in failed)
            raise HTTPException(
                status_code=400,
                detail=f"All repos failed to clone. Errors: {errors}",
            )

        # --- Step 2: Scan files from all repos ---
        code_extensions = {
            ext if ext.startswith(".") else f".{ext}"
            for ext in request.code_extensions
        }
        code_docs, doc_docs, _ = await connector.fetch_all(
            code_extensions=code_extensions,
        )

        if not code_docs and not doc_docs:
            stats = connector.get_combined_stats()
            raise HTTPException(
                status_code=400,
                detail=(
                    f"No code or doc files found across {stats['repos_cloned']} repo(s). "
                    f"Total files scanned: {stats['total_files']}. "
                    "Try adjusting `code_extensions` or `subdirectory`."
                ),
            )

        # --- Step 3: Parse ---
        all_code_chunks = []
        for doc in code_docs:
            file_path = doc.metadata.get("local_path", doc.source)
            chunks = parse_file(doc.content, file_path)
            all_code_chunks.extend(chunks)

        # --- Step 4: Chunk ---
        chunker = SemanticChunkerPipeline()
        semantic_code = chunker.chunk_code(all_code_chunks)
        semantic_docs = []
        for doc in doc_docs:
            semantic_docs.extend(
                chunker.chunk_document(doc.content, doc.source, doc.file_type)
            )

        all_semantic = semantic_code + semantic_docs

        # --- Step 5: Embed in ChromaDB ---
        store = VectorStore()
        chunks_to_upsert = [
            {"id": c.id, "content": c.content, "metadata": c.metadata}
            for c in all_semantic
        ]
        stored = store.upsert_chunks(request.skill_name, chunks_to_upsert)

        # --- Step 6: Run Council ---
        combined_text = "\n\n---\n\n".join(
            c.content for c in all_semantic[:50]
        )
        result = await run_council(combined_text, request.skill_name)
        result.chunks_ingested = stored

        return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Repo discovery error: {str(e)}")
    finally:
        # Always clean up all cloned repos
        if connector:
            connector.cleanup()


# ---------------------------------------------------------------------------
# Chat Endpoint
# ---------------------------------------------------------------------------

@app.post(
    "/api/chat",
    response_model=ChatResponse,
    tags=["Chat"],
    summary="Skill-Aware Chat",
    description=(
        "Ask a question using a specific Skill Hat. The AI will answer using:\n\n"
        "1. The skill's **SKILLS.md manifest** (architecture, rules, standards)\n"
        "2. **Relevant code/doc chunks** retrieved from ChromaDB via semantic search\n"
        "3. **Conversation history** for multi-turn context\n\n"
        "### Example Questions:\n"
        "- *\"What design patterns does the staking module use?\"*\n"
        "- *\"How should I implement a new reward calculator?\"*\n"
        "- *\"What validation rules apply when creating a stake?\"*\n"
        "- *\"Show me the correct error handling pattern\"*\n\n"
        "### Request Body:\n"
        "- `query`: Your question\n"
        "- `skill_hat`: Which skill to use (must exist ‚Äî run Discovery first)\n"
        "- `conversation_history`: Previous messages for multi-turn context (optional)"
    ),
    responses={
        404: {
            "model": ErrorResponse,
            "description": "Skill Hat not found. Run discovery for this skill first.",
        },
    },
)
async def chat(request: ChatRequest) -> ChatResponse:
    registry = SkillRegistry()
    if not registry.skill_exists(request.skill_hat):
        available = registry.list_skills()
        hint = f" Available skills: {available}" if available else " No skills generated yet."
        raise HTTPException(
            status_code=404,
            detail=(
                f"Skill '{request.skill_hat}' not found. Run discovery first.{hint}"
            ),
        )

    return await answer(request)


# ---------------------------------------------------------------------------
# Startup Event
# ---------------------------------------------------------------------------

@app.on_event("startup")
async def startup_event() -> None:
    """Log configuration on startup."""
    api_keys = validate_api_keys()
    registry = SkillRegistry()
    skills = registry.list_skills()
    data = _count_data_files()

    print()
    print("=" * 60)
    print("  üß† Nexus API ‚Äî v0.1.0")
    print("=" * 60)
    print()
    print("  API Keys:")
    print(f"    Google Gemini:  {'‚úÖ Ready' if api_keys['google'] else '‚ùå MISSING (required!)'}")
    print(f"    GitHub Token:   {'‚úÖ Ready' if api_keys['github'] else '‚¨ú Not set (using local files)'}")
    print(f"    Confluence:     {'‚úÖ Ready' if api_keys['confluence'] else '‚¨ú Not set (using local files)'}")
    print()
    print("  Data Folder:")
    print(f"    Code files:     {data['code_files']}")
    print(f"    Doc files:      {data['doc_files']}")
    print()
    print("  Skills:")
    print(f"    Generated:      {skills if skills else 'None (run POST /api/discover)'}")
    print()
    print("  Endpoints:")
    print("    Swagger UI:     http://localhost:8000/docs")
    print("    ReDoc:          http://localhost:8000/redoc")
    print("    Health:         http://localhost:8000/api/health")
    print()
    print("=" * 60)
    print()
